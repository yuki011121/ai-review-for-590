# 2025-10-17-5 Concise Randomized Hybrid Thesis Proposal Review process
#Teaching/Course/CSC-590/F25 #Date/2025/F25 
#Computer/Software/Canvas #Research/Peer-Review 

> [!Prompt] Title
> This is getting quite involved. Give me a comprehensive document with the instructions for this randomized combination and comparison of human and AI peer reviews for a thesis proposal. Start with an overview of the main steps and then describe the individual steps in more detail. Include the code for the scripts used.

This document outlines the **Randomized Hybrid Thesis Review Process** [[2025-10-17-4 Randomized Hybrid Thesis Review Process - Revised - Claude]] , which combines human peer review and Generative AI (GenAI) feedback for Master’s Thesis Proposals. The primary objective is to maintain **source blindness**—proposal authors must receive all reviews without knowing which were generated by humans (classmates) and which were generated by AI. The primary objective is to maintain **source blindness**—proposal authors must receive all reviews without knowing which were generated by humans (classmates) and which were generated by AI, turning the feedback evaluation into a critical assessment exercise.

This process leverages Canvas for submissions and automated scoring, paired with Python and shell scripts to automate randomization and file renaming for efficiency and error reduction.


---

## Randomized Hybrid Thesis Review Process: Instructions

### Overview of Main Steps

The process is divided into four stages, ensuring the required four reviews (two Human, two+ AI) are generated, randomized, distributed blindly, and assessed efficiently:

|Step|Title|Primary Tool/Responsibility|Output|Key Goal|
|:--|:--|:--|:--|:--|
|**1**|**Setup and Submission**|Canvas Automation|Internal Review Files (H1, H2)|Collect proposals; assign and collect anonymous human peer reviews (2 per proposal).|
|**2**|**GenAI Review Generation & Pre-Processing**|Instructor Overhead|Internal Review Files (AI1, AI2)|Generate relevant AI reviews (2+ per proposal).|
|**3**|**Automated Randomization & Key Generation**|Python Script|`Master_Key.csv`|Randomize the order of all four reviews (H1, H2, AI1, AI2) and create the internal key for tracking [Conversation History].|
|**4**|**Automated Renaming & Blind Distribution**|Shell Script & Canvas Gradebook|Blind Review Packages (e.g., `Review_1`, `Review_2`)|Rename files according to the randomized key and distribute the source-blind feedback package to the authors [Conversation History].|
|**5**|**Blind Feedback Assessment**|Canvas Quiz & Scoring|Prediction Data|Proposal writers analyze the feedback quality, predict the source (Human or AI), and justify their choice [Conversation History].|

---

### Detailed Steps and Procedures

#### Step 1: Setup and Submission (Canvas Automation)

This step uses Canvas to manage the thesis proposal submission and the anonymous pairing of human reviewers.

1. **Thesis Proposal Submission:** Students submit their Master's Thesis Proposal to a Canvas assignment (e.g., "Thesis Proposal"). Proposals must follow the specified structure, including components like the Abstract, Related Work, Design & Validation, and Hypothesis formulation.
2. **Canvas Peer Review Setup:** A separate assignment (e.g., "Thesis Proposal Peer Review") is configured to automatically and anonymously assign two peer reviews to each student.
    - Reviewers are instructed to mark up the proposal and provide a separate review form (0.5–1 page long) without including their name. These reviews should address content, style, and formatting criteria.
3. **Collect Human Reviews:** Once the peer review deadline passes, the instructor downloads all human peer review submissions.
4. **Internal Naming (H1, H2):** The human reviews are saved using a temporary internal format that identifies the student and source type (e.g., `S01_H1.pdf`, `S01_H2.pdf`).

#### Step 2: GenAI Review Generation & Pre-Processing (Instructor Overhead)

The instructor ensures the GenAI feedback is relevant by grounding the tools in course materials, as recommended by course policy on Generative AI use.

1. **Collect Submissions:** Download all original student proposals from the Canvas submission assignment.
2. **Grounding:** Ground the GenAI tools (e.g., Perplexity, ChatGPT, Claude) using key course documents:
    - The **"Thesis Proposal Template"** containing required structure (e.g., Title Page, Background, Evaluation and Validation Criteria).
    - The **Syllabus writing assignments** detailing expected content (e.g., Abstract, Thesis Intro, Design & Validation).
3. **Prompt Generation:** Generate at least two separate GenAI reviews for each proposal (AI1, AI2). The prompt should direct the AI to evaluate criteria such as hypothesis formulation and the validation plan.
4. **Internal Naming (AI1, AI2):** Save these AI-generated reviews using the temporary internal format (e.g., `S01_AI1.pdf`, `S01_AI2.pdf`).
5. **Placement:** Place all 4+ internal review files (`SXX_H1.pdf`, `SXX_H2.pdf`, `SXX_AI1.pdf`, `SXX_AI2.pdf`) into a designated source directory (e.g., `./reviews_original`).

#### Step 3: Automated Randomization & Key Generation (Python Script)

This script automates the creation of the essential tracking document (`Master_Key.csv`) by randomizing the public names assigned to the four internal review sources for every student, eliminating manual logging and error [Conversation History].

**Prerequisites:** Requires a file named `students.csv` listing all student IDs (e.g., `S01`, `S02`, etc.) and the presence of Python.

##### Python Script (`generate_master_key.py`):

```
import csv
import random

# --- Configuration ---
STUDENT_LIST_FILE = "students.csv"
MASTER_KEY_OUTPUT_FILE = "Master_Key.csv"
# Define internal source identifiers and their true source type
SOURCE_TYPES = {
    "H1": "Human",
    "H2": "Human",
    "AI1": "AI",
    "AI2": "AI"
}
# Define the generic public names students will see
PUBLIC_REVIEW_NAMES = [f"Review_{i}" for i in range(1, len(SOURCE_TYPES) + 1)]
# --- End Configuration ---

def generate_and_randomize_key():
    """Reads student list, randomizes review assignments, and generates the Master Key CSV."""

    master_key_data = []

    # 1. Read Student IDs
    try:
        with open(STUDENT_LIST_FILE, mode='r', newline='') as infile:
            reader = csv.reader(infile)
            next(reader)  # Skip header row (e.g., 'student_id')
            student_ids = [row.strip() for row in reader if row] # Assuming IDs are in the first column
    except FileNotFoundError:
        print(f"Error: {STUDENT_LIST_FILE} not found. Please ensure it exists.")
        return

    # 2. Iterate through students and generate randomized assignments
    for student_id in student_ids:

        internal_sources = list(SOURCE_TYPES.keys())

        # Create a randomized mapping for the public review names
        randomized_public_names = list(PUBLIC_REVIEW_NAMES)
        random.shuffle(randomized_public_names)

        # 3. Pair internal sources with randomized public names
        for i in range(len(internal_sources)):
            internal_id = internal_sources[i]
            public_name = randomized_public_names[i]
            true_source = SOURCE_TYPES[internal_id]

            # Internal filename (used by the subsequent shell script)
            internal_filename = f"{student_id}_{internal_id}.pdf"

            master_key_data.append({
                "Student_ID": student_id,
                "Internal_Name": internal_filename,
                "True_Source": true_source,
                "Public_Review_Name": public_name
            })

    # 4. Write the Master Key CSV file
    fieldnames = ["Student_ID", "Internal_Name", "True_Source", "Public_Review_Name"]
    with open(MASTER_KEY_OUTPUT_FILE, mode='w', newline='') as outfile:
        writer = csv.DictWriter(outfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(master_key_data)

    print(f"Successfully generated {MASTER_KEY_OUTPUT_FILE} for {len(student_ids)} students.")

if __name__ == "__main__":
    generate_and_randomize_key()
```

#### Step 4: Automated Renaming & Blind Distribution (Shell Script)

This script consumes the `Master_Key.csv` output from the Python script to rename and move the 112+ files into a blind, public format (e.g., `S01_Review_3.pdf`) [Conversation History].

##### Shell Script (`rename_and_distribute.sh`):

```
#!/bin/bash

# Configuration
MASTER_KEY_FILE="Master_Key.csv"
SOURCE_DIR="./reviews_original" # Contains S01_H1.pdf, etc.
TARGET_DIR="./reviews_blinded" # Output for S01_Review_1.pdf, etc.

# Create target directory if it doesn't exist
mkdir -p "$TARGET_DIR"

echo "Starting batch rename operation..."

# Skip the header row and read the Master Key CSV line by line
tail -n +2 "$MASTER_KEY_FILE" | while IFS=, read -r student_id internal_name true_source public_review_name; do

    # Trim whitespace for accurate file matching
    student_id=$(echo "$student_id" | xargs)
    internal_name=$(echo "$internal_name" | xargs)
    public_review_name=$(echo "$public_review_name" | xargs)

    # Construct paths
    SOURCE_PATH="${SOURCE_DIR}/${internal_name}"
    NEW_FILENAME="${student_id}_${public_review_name}.pdf"
    TARGET_PATH="${TARGET_DIR}/${NEW_FILENAME}"

    # Execute the rename/move operation
    if [ -f "$SOURCE_PATH" ]; then
        mv "$SOURCE_PATH" "$TARGET_PATH"
        echo "Renamed: ${internal_name} -> ${NEW_FILENAME}"
    else
        echo "Warning: Source file not found: ${SOURCE_PATH}"
    fi

done
echo "Batch renaming complete. Blinded files are in ${TARGET_DIR}."
```

#### Blind Feedback Distribution

1. **Packaging:** For each student ID, collect their four (or more) now-blindly-named files from the `TARGET_DIR` (e.g., `S01_Review_1.pdf`, `S01_Review_2.pdf`, etc.) and package them (e.g., ZIP file).
2. **Delivery:** Distribute the compiled package to the author via the **Canvas Gradebook** feature using **"Message Students Who..."** for the original "Thesis Proposal" assignment [Conversation History]. The message informs the student that they have received a complete set of feedback documents to use for revision.

#### Step 5: Blind Feedback Assessment (Canvas Quiz)

This step captures the quantifiable data on how accurately students can predict the source of the feedback they received, along with qualitative justification [Conversation History].

1. **Assignment Setup:** Create the "Review Feedback Assessment" as a structured **Canvas Quiz or Survey** [Conversation History].
2. **Quality Assessment (Essay Questions):** Include an Essay/Long Answer question asking students to compare and evaluate all received reviews based on standard criteria such as: Relevance, Accuracy, Succinctness, and Constructive Criticism.
3. **Blind Source Prediction (Multiple Choice):** For each review document received, pose a prediction question using a Multiple Choice format to automate data collection:
    - **Question (Review X):** "What was the source of Review X?"
        - A) Human Peer Reviewer
        - B) Generative AI Tool
    - Students must also provide justification in an accompanying essay field for their prediction, analyzing elements such as tone or language consistency [Conversation History].

#### Automated Scoring

1. **Export:** Download the Quiz results from Canvas (CSV/Excel) [Conversation History].
2. **Comparison:** Merge the exported student answers with the `Master_Key.csv` generated in Step 3. Use spreadsheet formulas to automatically compare the student's prediction (A or B) for each public review name (`Review_X`) against the `True_Source` listed in the Master Key. This calculation determines the accuracy of the identification without manual checking [Conversation History].



> [!Note] Perplexity Assessment
> [[2025-10-17-5b Feedback - Perplexity]]
